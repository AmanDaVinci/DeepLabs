# Lesson 1

## Key Points
---

- The notebooks have extra information and they let us experiment. Experimenting is the secret to developing a strong intuition for deep learning architectures and training!

- Learn on "as needed" basis. Do the entire course as fast as possible, then revisit again and again.

- CNN Image -> Structured Neural Network -> Language RNN -> Collaborative filtering intro -> Collaborative filtering in depth -> Structured Neural Network in depth -> CNN in depth -> Language RNN in depth

- Arthur Samuel did first machine learning with the checkers program that defeated himself. He had this idea of writing software through learning: Differential Learning

- Machine learning takes a lot of feature engineering and domain expertise

- Deep Learing: Infinitely flexible function (Universal Approximator Function), all-purpose parameter tuning (Gradient Descent Backpropagation), fast & scalable (GPU). Neural Network: non-linear function composed of cap Sigma, cap Pi and sigmoid function

- [IMP] Single hidden layer neural networks require exponentially huge number of parameters (neurons) to learn a function; however, with multiple hidden layers we get super linear scaling and that becomes deep learning. This means that it is easier to train a deep network than a shallow network with huge number of neurons.

- Too high learning rate can lead to divergence instead of convergence

- Linear function is anything with straight lines, planes or hyperplanes. The change/slope is always constant. It is a one degree polynomial equation. Matrix multiplications can be modelled as a linear function. However, non-linear functions have non-constant change/slope. It is a higher degree polynomial equation.

- One of the limitations of modern deep learning is that the GPUs have requirements that all images be of a standard size for it to be parallelizable and fast.

- For image classification tasks, resnet34 and resnet50 are always a good choice

- Pick one project and do it fantastically well

- Different layers of the neural network encode progressive levels of semantic complexity


## TODO
---

* Create your own problem of image classification : [how good/pro/viral is your photo?]
* Use doc() and help()
* Explore the documentation
* Experiment with perceptrons(no activation, linear) vs neurons (activations, non-linear)
* Data normalization vs batch normalization

- Understand Momentum and Learning Rate finder
- Setup blog on medium and personal website
- Design of Experiments
- Understand Log scale
- Understand Linear Visualizing Non-linear functions in maths
- Organize completed notebooks (Fog Detection and Fashion MNIST)
- Read source code with ??
- Setup infrastructure
- Experiment with a new dataset
- Learn Greek Letters


## Reading & Exploring
---

* Notebook: lesson1-pets

* Paper: Visualizing and Understanding Convulational Networks

* Notebook: lesson1-rxt50

* Blogs:
	* Neural Network in Numpy (https://sgugger.github.io/a-simple-neural-net-in-numpy.html#a-simple-neural-net-in-numpy)

- Notebook: lesson1-vgg


## Questions
---

